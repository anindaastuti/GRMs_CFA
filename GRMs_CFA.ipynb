{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3fcd1d5",
   "metadata": {
    "id": "c3fcd1d5"
   },
   "source": [
    "# Identifying gene regulation modules for tumor metastasis through combinatorial fusion analysis\n",
    "**Description: We developed an efficient bioinformatics approach to identify metastasis-associated\n",
    "gene regulatory modules (GRMs) in cancer networks. Using a subgraph method to extract\n",
    "GRMs, we applied combinatorial fusion analysis (CFA) to prioritize them based on relevance.** \\\n",
    "There are 7 main modules:\n",
    "- Module01: Calculate the LogFC and Hazard ratio\n",
    "- Module02: Count the occurences of KIRC genes in three databases\n",
    "- Module03: Cancer driver gene (COSMIS database)\n",
    "- Module04: CFA calculation Avg and Weigthed Score Combination (using 11 combinations of the features)\n",
    "- Module05: Find TOP2 and BOTTOM2 from Weigthed Score Combination result\n",
    "- Module06: Calculate frequency item-ID TOP5 and BOTTOM5. It is chosen from the highest and lowest five ranked items-Id for each of the 11 combinations.\n",
    "- Module07: Calculate Jaccard index to Quantify the difference between the weighted SC and average SC \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f682a9-0a5c-448a-8e8c-79126f261c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from itertools import islice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3b1f2",
   "metadata": {
    "id": "4ce3b1f2"
   },
   "source": [
    "## Subgraph Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556d32e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2565,
     "status": "ok",
     "timestamp": 1720693927821,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "0556d32e",
    "outputId": "21210aad-0e9b-4d04-a2df-2f0794773d08"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "# Define the working directory automatically\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Path to the directory containing all the pattern of three node subgraphs\n",
    "subgraph = \"/Users/anindaastuti/CFA Master/4node\"\n",
    "wkdr = \"/Users/anindaastuti/\"\n",
    "print(subgraph)\n",
    "print(wkdr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fc499ca",
   "metadata": {
    "id": "7fc499ca"
   },
   "source": [
    "# MODULE01: CALCULATE LogFC AND HAZARD RATIO: KIRC\n",
    "**Read the TMMGdb:** \\\n",
    "In order to get the full information from TMMGdb, the filtering conditions as follows: \n",
    "\n",
    "If |LogFC|< 0.38  \n",
    "\tThen  LogFC_label = 0\n",
    "Else if |LogFC| > 0.46   \n",
    "\tThen   LogFC_label = 10\n",
    "Else if |LogFC | >0.38 & <= 0.46\n",
    "\tThen   LogFC_label = Label\n",
    "\n",
    "If  HR < 1.05  \n",
    "\tThen  HR_label = 0\n",
    "Else if  HR > 1.20  \n",
    "\tThen   LogFC_label = 10\n",
    "Else if  HR >1.05 & <= 1.20   \n",
    "\tThen   LogFC_label = Label\n",
    "\n",
    " \n",
    "Total 25975 genes were collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec98a52",
   "metadata": {
    "id": "4ec98a52"
   },
   "source": [
    "### STEP01: HANDLE THE TMMGdb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5db77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1720693934121,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "86f5db77",
    "outputId": "0e3b0edb-7edb-4f88-8280-099e2c01592a"
   },
   "outputs": [],
   "source": [
    "#Read the TMMGdb-KIRC\n",
    "KIRC = pd.read_excel(f\"{wkdr}/data-tmmgdb-logFC-HR-pvalue-BRCA.xlsx\")\n",
    "print(KIRC.columns)\n",
    "\n",
    "\n",
    "#Rename columns\n",
    "KIRC.rename(columns={\"Adj-Pvalue.1\":\"Pvalue_HR\", \"Adj-Pvalue\":\"Pvalue_Log(FC)\"},inplace=True)\n",
    "print(KIRC.shape)\n",
    "\n",
    "#Create 2 Series: pvalue_series and HR_series\n",
    "mapping_pvalue_logFC  = dict(zip(KIRC['Gene'], KIRC['Pvalue_Log(FC)']))\n",
    "mapping_pvalue_HR     = dict(zip(KIRC['Gene'], KIRC['Pvalue_HR']))\n",
    "mapping_HR            = dict(zip(KIRC['Gene'], KIRC['HR']))\n",
    "mapping_logFC         = dict(zip(KIRC['Gene'], KIRC['Log(FC)']))\n",
    "\n",
    "print(f\"mapping_pvalue: {len(mapping_pvalue_logFC)}\")\n",
    "print(f\"mapping_Pvalue_HR: {len(mapping_pvalue_HR)}\")\n",
    "print(f\"mapping_HR: {len(mapping_HR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093888c5-294d-4b3f-9e3d-185fb8377a6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1720693938058,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "093888c5-294d-4b3f-9e3d-185fb8377a6a",
    "outputId": "bdb34b97-c667-4d93-e33d-f36e2252d1ba"
   },
   "outputs": [],
   "source": [
    "#abs means absolute\n",
    "#logFC_max_score = np.abs(KIRC['Log(FC)']).max()\n",
    "#logFC_min_score = np.abs(KIRC['Log(FC)']).min()\n",
    "logFC_min_score=0.38\n",
    "logFC_max_score=0.46\n",
    "print(\"logFC_max_score\",logFC_max_score)\n",
    "print(\"logFC_min_score\",logFC_min_score)\n",
    "# Calculate the range size\n",
    "n_range=10\n",
    "range_size = (logFC_max_score - logFC_min_score) / n_range\n",
    "print(\"range_size\",range_size)\n",
    "# Generate the range values\n",
    "#logFC_min_score + i * range_size+0.000001 digunakan untuk membuat batas bawah\n",
    "#ketika index i =0 maka min score adalah logFC_min_score\n",
    "#if i == 0 digunakan untuk nilai awal adalah 0.38 dan selanjutnya di tambah 0.000001\n",
    "#logFC_min_score + (i + 1) * range_size digunakan untuk membuat batas atas\n",
    "#ketika index i =0 maka max score adalah logFC_min_score+range_size\n",
    "ranges = [(logFC_min_score + i * range_size if i == 0 else logFC_min_score + i * range_size + 0.001, logFC_min_score + (i + 1) * range_size) for i in range(n_range)]\n",
    "\n",
    "# Labels for the ranges\n",
    "labels = [i+1 for i in range(n_range)]\n",
    "\n",
    "# Create a list of tuples containing the range labels, maximum and minimum values\n",
    "#zip digunakan untuk menggabungkan 2 list data\n",
    "#(label, r[0], r[1]) digunakan untuk membuat setiap baris data, label: 1-5, r[0]: batas bawah, r[1]: batas atas\n",
    "#for label, r in zip(labels, ranges) digunakan untuk mengekstrak data label dan juga ranges\n",
    "# r adalah data ranges\n",
    "range_data = [(label, r[0], r[1]) for label, r in zip(labels, ranges)]\n",
    "\n",
    "# Create a DataFrame from the list of tuples\n",
    "range_df = pd.DataFrame(range_data, columns=['LogFC_Label', 'LogFC_Min', 'LogFC_Max'])\n",
    "\n",
    "print(range_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab0bb9-e1c1-42c8-9f46-efcc33694b95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1720693943146,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "4bab0bb9-e1c1-42c8-9f46-efcc33694b95",
    "outputId": "0e7a71d6-439f-461b-9935-4fad6473fb44"
   },
   "outputs": [],
   "source": [
    "#HR_max_score = KIRC['HR'].max()\n",
    "#HR_min_score = KIRC['HR'].min()\n",
    "HR_max_score=1.20\n",
    "HR_min_score=1.05\n",
    "print(\"HR_max_score\",HR_max_score)\n",
    "print(\"HR_min_score\",HR_min_score)\n",
    "# Calculate the range size\n",
    "n_range_HR=10\n",
    "HR_range_size = (HR_max_score - HR_min_score) / n_range_HR\n",
    "print(\"HR_range_size\",HR_range_size)\n",
    "# Generate the range values\n",
    "HR_ranges = [(HR_min_score + i * HR_range_size if i == 0 else HR_min_score + i * HR_range_size+0.001, HR_min_score + (i + 1) * HR_range_size) for i in range(n_range_HR)]\n",
    "\n",
    "# Labels for the ranges\n",
    "HR_labels = [i+1 for i in range(n_range_HR)]\n",
    "\n",
    "# Create a list of tuples containing the range labels, maximum and minimum values\n",
    "HR_range_data = [(label, r[0], r[1]) for label, r in zip(HR_labels, HR_ranges)]\n",
    "\n",
    "# Create a DataFrame from the list of tuples\n",
    "HR_range_df = pd.DataFrame(HR_range_data, columns=['HR_Label', 'HR_Min', 'HR_Max'])\n",
    "\n",
    "print(HR_range_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af165f0e",
   "metadata": {
    "id": "af165f0e"
   },
   "source": [
    "### Step 02: Calculate P-value and Hazard ratio of each node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fc6a4",
   "metadata": {
    "id": "4b1fc6a4"
   },
   "source": [
    "**Process through each 4-subgraph network type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d82bb7-89d0-4a9b-b902-55e1e89743bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1619,
     "status": "ok",
     "timestamp": 1720693949857,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "d4d82bb7-89d0-4a9b-b902-55e1e89743bc",
    "outputId": "25e846ba-a884-4aee-de26-b550586f2eae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Assuming you have defined mapping_pvalue, mapping_LogFC, and mapping_HR functions\n",
    "\n",
    "\n",
    "#folder_path = \"/Users/anindaastuti/CFA Master Code-breast cancer [BRCA]/4node\"\n",
    "\n",
    "\n",
    "default_value_for_logFC=0\n",
    "default_value_for_HR=0\n",
    "# Create a new Excel workbook\n",
    "print(\"cek data\")\n",
    "outputfolder = f\"{wkdr}/output files\"\n",
    "output_file_path = f\"{outputfolder}/result_Pvalue-HR.xlsx\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = [os.path.join(subgraph, f) for f in os.listdir(subgraph) if os.path.isfile(os.path.join(subgraph, f))]\n",
    "\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    for subgraph_file in all_files:\n",
    "        print(f\"Reading file: {subgraph_file}\")\n",
    "        with open(subgraph_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "            print(file.read())\n",
    "        \n",
    "        subgraph_name = os.path.basename(subgraph_file).split(\".\")[0]\n",
    "        print(subgraph_name)\n",
    "        subgraph_data = pd.read_csv(subgraph_file, sep=\"\\t\", skiprows=1, header=None)\n",
    "        \n",
    "        # Removing the last row\n",
    "        subgraph_data = subgraph_data.drop(subgraph_data.index[-1])\n",
    "\n",
    "        # Removing whitespaces in dataframe\n",
    "        subgraph_data = subgraph_data.replace('\\\\s+', '', regex=True)\n",
    "\n",
    "        # Split the original column into 3 columns named: gene1, gene2, gene3\n",
    "        subgraph_data[[\"gene1\", \"gene2\", \"gene3\",\"gene4\"]] = subgraph_data[0].str.split(\"|\", expand=True)\n",
    "        subgraph_data[\"0\"]=subgraph_data[0]\n",
    "        # Adding Pvalue and HR of each gene into the dataframe\n",
    "        for gene in [\"gene1\", \"gene2\", \"gene3\",\"gene4\"]:\n",
    "            subgraph_data[\"logFC:\" + gene] = subgraph_data[gene].map(mapping_logFC)\n",
    "            subgraph_data[\"logFC_adj_p:\" + gene] = subgraph_data[gene].map(mapping_pvalue_logFC)\n",
    "            subgraph_data[\"HR:\" + gene] = subgraph_data[gene].map(mapping_HR)\n",
    "            subgraph_data[\"HR_adj_p:\" + gene] = subgraph_data[gene].map(mapping_pvalue_HR)\n",
    "\n",
    "        # Calculate the compounded P-value and compounded HR if needed\n",
    "        # ...\n",
    "\n",
    "        # Other calculations\n",
    "\n",
    "\n",
    "# Assuming 'gene' is a variable containing the gene name\n",
    "            # abs means absolute value\n",
    "            subgraph_data[\"logFC_adj_p<0.05:\" + gene] = np.where((subgraph_data[\"logFC_adj_p:\" + gene] < 0.05), 1, 0)\n",
    "\n",
    "            subgraph_data[\"HR_adj_p<0.05:\" + gene] = np.where((subgraph_data[\"HR_adj_p:\" + gene] < 0.05), 1, 0)\n",
    "\n",
    "\n",
    "            # Create empty lists to store the range labels\n",
    "            logFC_range_labels = []\n",
    "            HR_range_labels = []\n",
    "\n",
    "            # LogFC Calculation\n",
    "            # Iterate over each row of subgraph_data\n",
    "            for index, row in subgraph_data.iterrows():\n",
    "                logFC_value = np.abs(row[\"logFC:\" + gene])\n",
    "                HR_value = row[\"HR:\" + gene]\n",
    "                 # Take only 4 significant figures\n",
    "                logFC_value = float(f\"{logFC_value:.4g}\")\n",
    "\n",
    "                HR_value = float(f\"{HR_value:.4g}\")\n",
    "    # Check which range label the logFC_value belongs to\n",
    "                logFC_label_found = False\n",
    "                if logFC_value < logFC_min_score:\n",
    "                    logFC_range_labels.append(0)\n",
    "                    logFC_label_found = True\n",
    "                elif logFC_value >logFC_max_score:\n",
    "                    logFC_range_labels.append(10)\n",
    "                    logFC_label_found = True\n",
    "                else:\n",
    "                    for _, range_row in range_df.iterrows():\n",
    "                        if range_row[\"LogFC_Min\"] <= logFC_value <= range_row[\"LogFC_Max\"]:\n",
    "                            logFC_range_labels.append(range_row[\"LogFC_Label\"])\n",
    "                            logFC_label_found = True\n",
    "                            print(\"logFC_check:\",logFC_value)\n",
    "                            print(\"logFC_range_labels:\",logFC_range_labels)\n",
    "                            #break\n",
    "\n",
    "  # Check if a label was found, if not, append a default value\n",
    "                if not logFC_label_found:\n",
    "                    logFC_range_labels.append(default_value_for_logFC)\n",
    "\n",
    "                # Check which range label the HR_value belongs to\n",
    "                HR_label_found = False\n",
    "                if HR_value < HR_min_score:\n",
    "                    HR_range_labels.append(0)\n",
    "                    HR_label_found = True\n",
    "                elif HR_value > HR_max_score:\n",
    "                    HR_range_labels.append(10)\n",
    "                    HR_label_found = True\n",
    "                else:\n",
    "                    for _, HR_range_row in HR_range_df.iterrows():\n",
    "                        if HR_range_row[\"HR_Min\"] <= HR_value <= HR_range_row[\"HR_Max\"]:\n",
    "                            HR_range_labels.append(HR_range_row[\"HR_Label\"])\n",
    "                            HR_label_found = True\n",
    "                            #break\n",
    "\n",
    "              # Check if a label was found, if not, append a default value\n",
    "                if not HR_label_found:\n",
    "                  HR_range_labels.append(default_value_for_HR)\n",
    "\n",
    "# After the loop, assign the lists to DataFrame columns\n",
    "            subgraph_data[\"logFC_Label:\" + gene] = logFC_range_labels\n",
    "            print(\"result LogFC Label:\",subgraph_data[\"logFC_Label:\" + gene])\n",
    "            subgraph_data[\"HR_Label:\" + gene] = HR_range_labels\n",
    "        subgraph_data['Total_Combination_logFC'] = subgraph_data[['logFC_Label:gene1','logFC_adj_p<0.05:gene1','logFC_Label:gene2','logFC_adj_p<0.05:gene2','logFC_Label:gene3','logFC_adj_p<0.05:gene3','logFC_Label:gene4','logFC_adj_p<0.05:gene4']].astype(int).sum(axis=1)\n",
    "        subgraph_data['Total_Combination_HR'] = subgraph_data[['HR_Label:gene1','HR_adj_p<0.05:gene1','HR_Label:gene2','HR_adj_p<0.05:gene2','HR_Label:gene3','HR_adj_p<0.05:gene3','HR_Label:gene4','HR_adj_p<0.05:gene4']].astype(int).sum(axis=1)\n",
    "\n",
    "\n",
    "        # Rearrange the columns in subgraph_data as per your preference\n",
    "        subgraph_data = subgraph_data[[\n",
    "            # Place the columns you want in the desired order\n",
    "            '0',\n",
    "            'gene1', 'gene2', 'gene3','gene4',\n",
    "            # Other columns...\n",
    "            'logFC:gene1','logFC_adj_p:gene1','HR:gene1','HR_adj_p:gene1',\n",
    "            'logFC:gene2','logFC_adj_p:gene2','HR:gene2','HR_adj_p:gene2',\n",
    "            'logFC:gene3','logFC_adj_p:gene3','HR:gene3','HR_adj_p:gene3',\n",
    "            'logFC:gene4','logFC_adj_p:gene4','HR:gene4','HR_adj_p:gene4',\n",
    "            'logFC_Label:gene1','logFC_adj_p<0.05:gene1','logFC_Label:gene2','logFC_adj_p<0.05:gene2','logFC_Label:gene3','logFC_adj_p<0.05:gene3','logFC_Label:gene4','logFC_adj_p<0.05:gene4',\n",
    "            'HR_Label:gene1','HR_adj_p<0.05:gene1','HR_Label:gene2','HR_adj_p<0.05:gene2','HR_Label:gene3','HR_adj_p<0.05:gene3','HR_Label:gene4','HR_adj_p<0.05:gene4',\n",
    "            'Total_Combination_logFC','Total_Combination_HR'\n",
    "        ]]\n",
    "\n",
    "        print(subgraph_data)\n",
    "\n",
    "        # Save the processed dataframe to excel file\n",
    "        subgraph_data.to_excel(writer, sheet_name=subgraph_name, index=False)\n",
    "# Print the path of the created Excel file\n",
    "print(f\"Excel file created at: {output_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c542c2-5575-4026-bdeb-72824f620882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Check LogFC and HR that cannot find in TMMGDB\n",
    "\n",
    "\n",
    "# List of columns to check for non-empty values\n",
    "columns_to_check = [\n",
    "    'logFC:gene1', 'logFC_adj_p:gene1', 'HR:gene1', 'HR_adj_p:gene1',\n",
    "    'logFC:gene2', 'logFC_adj_p:gene2', 'HR:gene2', 'HR_adj_p:gene2',\n",
    "    'logFC:gene3', 'logFC_adj_p:gene3', 'HR:gene3', 'HR_adj_p:gene3',\n",
    "    'logFC:gene4', 'logFC_adj_p:gene4', 'HR:gene4', 'HR_adj_p:gene4'\n",
    "]\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = pd.ExcelFile(output_file_path)\n",
    "\n",
    "# Dictionary to store filtered data from each sheet\n",
    "filtered_data = {}\n",
    "consolidated_df = pd.DataFrame()  # DataFrame to store merged data\n",
    "\n",
    "# Iterate through each sheet\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    \n",
    "    # Check if all specified columns are present in the sheet\n",
    "    if all(col in df.columns for col in columns_to_check):\n",
    "        # Filter rows where all columns are non-empty\n",
    "        filtered_df = df.dropna(subset=columns_to_check)\n",
    "        \n",
    "        # Store the filtered data\n",
    "        filtered_data[sheet_name] = filtered_df\n",
    "\n",
    "        # Print the sheet name and the filtered data\n",
    "        print(f\"\\nFiltered data for sheet: {sheet_name}\")\n",
    "        if filtered_df.empty:\n",
    "            print(\"No rows found with all non-empty values.\")\n",
    "        else:\n",
    "            print(filtered_df)\n",
    "\n",
    "        # Append to the consolidated DataFrame\n",
    "        consolidated_df = pd.concat([consolidated_df, filtered_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Output filtered data or save to a new Excel file\n",
    "output_filtered_file = f\"{outputfolder}/filtered_LogFC_HR_result_Pvalue-HR.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_filtered_file) as writer:\n",
    "    for sheet_name, filtered_df in filtered_data.items():\n",
    "        # Save each sheet to the new Excel file\n",
    "        filtered_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Save the consolidated data as the last sheet\n",
    "    consolidated_df.to_excel(writer, sheet_name=\"Consolidated\", index=False)\n",
    "\n",
    "\n",
    "print(\"\\nFiltered data saved to 'filtered_LogFC_HR_result_Pvalue-HR.xlsx'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ad21e",
   "metadata": {
    "id": "813ad21e"
   },
   "source": [
    "# Module02: Count the occurences of KIRC genes in 3 databases\n",
    "### Step01: Convert GeneID to GeneSymbol\n",
    "\n",
    "Download database\n",
    "- The original database: https://www.genenames.org/, contains the source for approved human gene nomenclature\n",
    "- The database is located at this site: https://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/tsv/\n",
    "- Downloading process takes around 1 minutes, if it takes more time than I suggest, please check for the other errors.\n",
    "- If we fail download the file, we can use human_nomenclature_gene.txt that we already downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf59f76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2054,
     "status": "ok",
     "timestamp": 1720693960906,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "0bf59f76",
    "outputId": "443b6f9a-600b-48a6-a3a7-c2c43d2949df"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the file to download\n",
    "url = \"https://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/tsv/non_alt_loci_set.txt\"\n",
    "\n",
    "# Define the destination directory\n",
    "destination_directory = wkdr\n",
    "\n",
    "# Send a GET request to the URL to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Open the destination file in binary write mode and write the downloaded content\n",
    "    with open(f\"{destination_directory}/human_nomenclature_gene.txt\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cea41",
   "metadata": {
    "id": "0d9cea41"
   },
   "source": [
    "**Handle the human nomenclature database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2805154",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1720693966958,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "b2805154",
    "outputId": "589ba653-01c2-4c87-b4a5-e79fe3949491"
   },
   "outputs": [],
   "source": [
    "# Read in the dataframe\n",
    "\n",
    "# Define the destination directory\n",
    "destination_directory = wkdr\n",
    "\n",
    "human_nomenclature_gene = pd.read_csv(f\"{destination_directory}/human_nomenclature_gene.txt\",sep = \"\\t\")\n",
    "\n",
    "# Taking only 2 columns into consideration: symbol and entrez_id\n",
    "human_nomenclature_gene = human_nomenclature_gene[[\"symbol\",\"entrez_id\"]]\n",
    "\n",
    "# Replace the Null value by 0\n",
    "human_nomenclature_gene['entrez_id'] = human_nomenclature_gene['entrez_id'].fillna(0)\n",
    "\n",
    "# Convert the type of values in \"entrez_id\" columns from float64 to int\n",
    "human_nomenclature_gene[\"entrez_id\"] = human_nomenclature_gene[\"entrez_id\"].astype(int)\n",
    "print(human_nomenclature_gene)\n",
    "\n",
    "# Convert the type of human_nomenclature_gene: From dataframe to Dictionary\n",
    "nomenclature_gene_dict = human_nomenclature_gene.set_index('entrez_id')['symbol'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cddf46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1720693972066,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "c3cddf46",
    "outputId": "d79b2b5c-f8fd-492b-a3fa-0e904b180d53"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the working directory\n",
    "\n",
    "# Get a list of all files in the folder DB\n",
    "databases = f\"{wkdr}/DBs\"\n",
    "databases_files = [os.path.join(databases, f) for f in os.listdir(databases) if os.path.isfile(os.path.join(databases, f))]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "total_DB = pd.DataFrame()\n",
    "\n",
    "# Loop through each database file\n",
    "for DB_file in databases_files:\n",
    "    print(f\"Reading file: {DB_file}\")\n",
    "\n",
    "    # Get the database name (file name without extension)\n",
    "    database_name = os.path.basename(DB_file).replace(\".txt\", \"\")\n",
    "\n",
    "    # Skip if database_name is \"data-Over3GeneList\"\n",
    "    if database_name == \"data-Over3GeneList\":\n",
    "        continue  # Skip this file\n",
    "\n",
    "    # Read the database file into a DataFrame\n",
    "    df = pd.read_csv(DB_file, header=None)\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={0: \"GenID\"}, inplace=True)\n",
    "\n",
    "    # Add a new column with value 1 for this database\n",
    "    df[database_name] = 1  \n",
    "\n",
    "    # Set \"GenID\" as the index\n",
    "    df.set_index(\"GenID\", inplace=True)\n",
    "\n",
    "    # Merge databases by adding new columns\n",
    "    total_DB = pd.concat([total_DB, df], axis=1)  # Merge by columns\n",
    "\n",
    "# Fill missing values with 0\n",
    "total_DB.fillna(0, inplace=True)\n",
    "\n",
    "# Map nomenclature gene names\n",
    "total_DB[\"GenSymbol\"] = total_DB.index.map(nomenclature_gene_dict)\n",
    "\n",
    "# Ensure that the required columns exist before filtering\n",
    "required_columns = [\"GenSymbol\", \"CMGene\", \"HCMDB\", \"TMMGdb_3200\"]\n",
    "for col in required_columns:\n",
    "    if col not in total_DB.columns:\n",
    "        total_DB[col] = 0  # Add missing columns with default value 0\n",
    "\n",
    "# Keep only relevant columns\n",
    "total_DB = total_DB[required_columns]\n",
    "\n",
    "# Calculate occurrences by summing across selected columns\n",
    "total_DB[\"Occurrences\"] = total_DB[['CMGene', 'HCMDB', 'TMMGdb_3200']].sum(axis=1)\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(total_DB)\n",
    "\n",
    "# Convert to dictionary with \"GenSymbol\" as key and \"Occurrences\" as value\n",
    "total_DB_dict = total_DB.set_index('GenSymbol')['Occurrences'].to_dict()\n",
    "print(total_DB_dict)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(list(total_DB_dict.items()), columns=['Sheet Name', 'Order'])\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv(f\"{wkdr}/output files/total_OCCinThreeDB_dict.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df05260b",
   "metadata": {
    "id": "df05260b"
   },
   "source": [
    "### Step02: Count the occurrences of KIRC genes in databases\n",
    "**Read-in the excel file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753f6e7",
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1720693977720,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "1753f6e7"
   },
   "outputs": [],
   "source": [
    "path_to_the_KIRC_gene = f\"{wkdr}/output files/result_Pvalue-HR.xlsx\"\n",
    "path_to_the_KIRC_gene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d7b05",
   "metadata": {
    "id": "c02d7b05"
   },
   "source": [
    "**Handle one-by-one sheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9497a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1720693979705,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "5ab9497a",
    "outputId": "871283b2-83b3-4d02-be34-d6f326a9e836"
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(f\"{wkdr}/output files/result_other_databases.xlsx\") as writer:\n",
    "    analyzed_subgraph = pd.read_excel(path_to_the_KIRC_gene, sheet_name=None)\n",
    "    print(analyzed_subgraph)\n",
    "\n",
    "    for sheet_name, processing_sheet in analyzed_subgraph.items():\n",
    "        # Process each sheet as needed\n",
    "        print(f\"Sheet Name: {sheet_name}\")\n",
    "\n",
    "        # Map the gene in the network to total_DB_dict in order to get the occurences of this gene in 3 databases\n",
    "        processing_sheet[\"Occurences:gene1\"] = processing_sheet[\"gene1\"].map(total_DB_dict)\n",
    "        processing_sheet[\"Occurences:gene2\"] = processing_sheet[\"gene2\"].map(total_DB_dict)\n",
    "        processing_sheet[\"Occurences:gene3\"] = processing_sheet[\"gene3\"].map(total_DB_dict)\n",
    "        processing_sheet[\"Occurences:gene4\"] = processing_sheet[\"gene4\"].map(total_DB_dict)\n",
    "\n",
    "        #Fill NA will O\n",
    "        processing_sheet.fillna(0,inplace=True)\n",
    "\n",
    "        #Sum the occurences of 3 genes in 3 database\n",
    "        processing_sheet[\"Total_Occurences\"] = processing_sheet[[\"Occurences:gene1\",\"Occurences:gene2\",\"Occurences:gene3\",\"Occurences:gene4\"]].sum(axis=1)\n",
    "\n",
    "        processing_sheet.to_excel(writer,sheet_name = sheet_name,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff90f74",
   "metadata": {
    "id": "fff90f74"
   },
   "source": [
    "# MODULE03: CANCER DRIVER GENES THE COSMIS DATABASE\n",
    "**Description:**\n",
    "Each subgraph contains 3 genes, the question is how many within 3 genes are presented in COSMIC database\n",
    "\n",
    "Tier01: https://cancer.sanger.ac.uk/cosmic/census?tier=1\n",
    "\n",
    "### Step01: Read the Cosmic database (Tier01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb158d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1720693986604,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "10cb158d",
    "outputId": "a16274ee-bc20-4bd4-c86b-0e3620f67be3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosmic = pd.read_csv(f\"{wkdr}/Census_Tier1.tsv\",sep=\"\\t\")\n",
    "\n",
    "cosmic[\"Cosmic_Tier1\"] = 1\n",
    "\n",
    "cosmic.rename(columns={\"Gene Symbol\":\"GenSymbol\"},inplace=True)\n",
    "cosmic = cosmic[[\"GenSymbol\",\"Cosmic_Tier1\"]]\n",
    "\n",
    "cosmic = cosmic.set_index('GenSymbol')['Cosmic_Tier1'].to_dict()\n",
    "print(cosmic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3939f0f",
   "metadata": {
    "id": "c3939f0f"
   },
   "source": [
    "### Step03: Count the occurrence of genes in each subgraph in COSMIC database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4259a43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1720693993226,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "f4259a43",
    "outputId": "69d7d48f-dbd5-405a-c53a-98d257363f5b"
   },
   "outputs": [],
   "source": [
    "Occurrence_sheet = f\"{wkdr}/output files/result_other_databases.xlsx\"\n",
    "with pd.ExcelWriter(f\"{wkdr}/output files/result_Cosmic.xlsx\") as writer:\n",
    "\n",
    "    analyzed_subgraph = pd.read_excel(Occurrence_sheet, sheet_name=None)\n",
    "\n",
    "    for sheet_name, processing_sheet in analyzed_subgraph.items():\n",
    "        print(sheet_name)\n",
    "        #print(processing_sheet)\n",
    "        # Map the gene in the network to total_DB_dict in order to get the occurences of this gene in 3 databases\n",
    "        processing_sheet[\"Cosmic:gene1\"] = processing_sheet[\"gene1\"].map(cosmic)\n",
    "        processing_sheet[\"Cosmic:gene2\"] = processing_sheet[\"gene2\"].map(cosmic)\n",
    "        processing_sheet[\"Cosmic:gene3\"] = processing_sheet[\"gene3\"].map(cosmic)\n",
    "        processing_sheet[\"Cosmic:gene4\"] = processing_sheet[\"gene4\"].map(cosmic)\n",
    "        \n",
    "        processing_sheet = processing_sheet.rename(columns={\"0\":\"index\"})\n",
    "\n",
    "        #Fill NA with 0\n",
    "        processing_sheet.fillna(0,inplace=True)\n",
    "\n",
    "        #Sum the occurences of 4 genes in 3 database\n",
    "        processing_sheet[\"Total_Cosmic\"] = processing_sheet[[\"Cosmic:gene1\",\"Cosmic:gene2\",\"Cosmic:gene3\",\"Cosmic:gene4\"]].sum(axis=1)\n",
    "\n",
    "        processing_sheet.to_excel(writer,sheet_name = sheet_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7293e14-b4e2-498c-ba5e-88f1e135a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processing_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971c09a-bdd3-4266-abb9-fb40a40327d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47916988-c27e-4ec2-bb89-8c63d51d7376",
   "metadata": {
    "id": "47916988-c27e-4ec2-bb89-8c63d51d7376"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6011950",
   "metadata": {
    "id": "a6011950"
   },
   "source": [
    "# MODULE04: CFA calculation Avg and Weigthed Score Combination (using 11 combinations of the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f3899-339d-4447-84a3-e9be41b97979",
   "metadata": {
    "id": "8a0f3899-339d-4447-84a3-e9be41b97979"
   },
   "outputs": [],
   "source": [
    "def re_normalize_column(x):\n",
    "    re_value = (x - min(x)) / (max(x) - min(x))\n",
    "    return re_value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cceadd4-ebb3-4c80-b4d0-fe526d4c3959",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 918,
     "status": "ok",
     "timestamp": 1715952171029,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "8cceadd4-ebb3-4c80-b4d0-fe526d4c3959",
    "outputId": "26dd0b9d-8da7-4df0-b1dc-74f4a91aca3a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rank_score_combination(input_sheet, output_file):\n",
    "    # Define the desired order of sheets\n",
    "    file_path = f\"{wkdr}/output files/result_Cosmic.xlsx\"\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create a dictionary mapping sheet names to their order\n",
    "    sheet_order = {sheet_name: index for index, sheet_name in enumerate(xls.sheet_names)}\n",
    "\n",
    "# Print the sheet order dictionary\n",
    "    print(sheet_order)\n",
    "    \n",
    "    # Initialize an empty list to store data from all sheets\n",
    "    combined_data = []\n",
    "    \n",
    "    # Read the sheet:\n",
    "    analyzed_subgraph = pd.read_excel(input_sheet, sheet_name=None)\n",
    "    \n",
    "    # Sort the sheets by the desired order and combine data\n",
    "    for sheet_name in sheet_order:\n",
    "        if sheet_name in analyzed_subgraph:\n",
    "            print(f\"The sheet_id {sheet_name} is in processing\")\n",
    "            processing_sheet = analyzed_subgraph[sheet_name]\n",
    "            processing_sheet[\"Sheet_Name\"] = sheet_name  # Add a column to identify the source sheet\n",
    "            combined_data.append(processing_sheet)\n",
    "    \n",
    "    # Concatenate all the sheets into a single DataFrame\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    \n",
    "    # Step 1: Scores normalization\n",
    "    combined_df[\"slogFC\"] = combined_df[\"Total_Combination_logFC\"] / ((10*4) + 4)\n",
    "    combined_df[\"sHR\"] = combined_df[\"Total_Combination_HR\"] / ((10*4) + 4)\n",
    "    combined_df[\"sOcc\"] = combined_df[\"Total_Occurences\"] / 12\n",
    "    combined_df[\"sCDG\"] = combined_df[\"Total_Cosmic\"] / 4\n",
    "    \n",
    "    # Calculate score combination and rank\n",
    "    combined_df[\"SC\"] = combined_df[[\"slogFC\", \"sHR\", \"sOcc\", \"sCDG\"]].sum(axis=1)\n",
    "    combined_df[\"Final Rank\"] = combined_df[\"SC\"].rank(ascending=False, method='average')\n",
    "    \n",
    "    # Re-normalize the columns\n",
    "    combined_df[\"slogFC'\"] = re_normalize_column(combined_df[\"slogFC\"])\n",
    "    combined_df[\"sHR'\"] = re_normalize_column(combined_df[\"sHR\"])\n",
    "    combined_df[\"sOcc'\"] = re_normalize_column(combined_df[\"sOcc\"])\n",
    "    combined_df[\"sCDG'\"] = re_normalize_column(combined_df[\"sCDG\"])\n",
    "    \n",
    "    # Calculate re-normalized score combination and rank\n",
    "    combined_df[\"SC'\"] = combined_df[[\"slogFC'\", \"sHR'\", \"sOcc'\", \"sCDG'\"]].sum(axis=1)\n",
    "    combined_df[\"Final Rank'\"] = combined_df[\"SC'\"].rank(ascending=False, method='average')\n",
    "    \n",
    "    # Ranking the columns\n",
    "    combined_df[\"rlogFC\"] = combined_df[\"slogFC'\"].rank(ascending=False, method='dense')\n",
    "    combined_df[\"rHR\"] = combined_df[\"sHR'\"].rank(ascending=False, method='dense')\n",
    "    combined_df[\"rOcc\"] = combined_df[\"sOcc'\"].rank(ascending=False, method='dense')\n",
    "    combined_df[\"rCDG\"] = combined_df[\"sCDG'\"].rank(ascending=False, method='dense')\n",
    "    \n",
    "    # Keep track of the gene\n",
    "    combined_df[\"flogFC\"] = combined_df[\"slogFC'\"].reset_index(drop=True).sort_values(ascending=False).values\n",
    "    \n",
    "    sorted_indices = np.argsort(-combined_df[\"slogFC'\"])\n",
    "    sorted_index_labels = combined_df[\"index\"].iloc[sorted_indices].index + 1\n",
    "    sorted_index_labels_with_d = ['d' + str(label) for label in sorted_index_labels]\n",
    "    combined_df[\"D_logFC\"] = sorted_index_labels_with_d\n",
    "    \n",
    "    combined_df[\"fHR\"] = combined_df[\"sHR'\"].reset_index(drop=True).sort_values(ascending=False).values\n",
    "    sorted_indices = np.argsort(-combined_df[\"sHR'\"])\n",
    "    sorted_index_labels = combined_df[\"index\"].iloc[sorted_indices].index + 1\n",
    "    sorted_index_labels_with_d = ['d' + str(label) for label in sorted_index_labels]\n",
    "    combined_df[\"D_HR\"] = sorted_index_labels_with_d\n",
    "    \n",
    "    combined_df[\"fOcc\"] = combined_df[\"sOcc'\"].reset_index(drop=True).sort_values(ascending=False).values\n",
    "    sorted_indices = np.argsort(-combined_df[\"sOcc'\"])\n",
    "    sorted_index_labels = combined_df[\"index\"].iloc[sorted_indices].index + 1\n",
    "    sorted_index_labels_with_d = ['d' + str(label) for label in sorted_index_labels]\n",
    "    combined_df[\"D_Occ\"] = sorted_index_labels_with_d\n",
    "    \n",
    "    combined_df[\"fCDG\"] = combined_df[\"sCDG'\"].reset_index(drop=True).sort_values(ascending=False).values\n",
    "    sorted_indices = np.argsort(-combined_df[\"sCDG'\"])\n",
    "    sorted_index_labels = combined_df[\"index\"].iloc[sorted_indices].index + 1\n",
    "    sorted_index_labels_with_d = ['d' + str(label) for label in sorted_index_labels]\n",
    "    combined_df[\"D_CDG\"] = sorted_index_labels_with_d\n",
    "    \n",
    "    # Save the combined results to a new Excel file\n",
    "    combined_df.to_excel(output_file, sheet_name=\"Combined_Results\", index=False)\n",
    "    \n",
    "# Example usage:\n",
    "input_data = f\"{wkdr}/output files/result_Cosmic.xlsx\"\n",
    "output_data = f\"{wkdr}/output files/combined-rank-score-combination.xlsx\"\n",
    "rank_score_combination(input_data, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f5d46-d3df-4a22-bc84-1f633721017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Check LogFC,HR,OCC,CDG that cannot find in TMMGDB\n",
    "\n",
    "\n",
    "# List of columns to check for non-empty values\n",
    "columns_to_check = [\n",
    "    'logFC:gene1', 'logFC_adj_p:gene1', 'HR:gene1', 'HR_adj_p:gene1',\n",
    "    'logFC:gene2', 'logFC_adj_p:gene2', 'HR:gene2', 'HR_adj_p:gene2',\n",
    "    'logFC:gene3', 'logFC_adj_p:gene3', 'HR:gene3', 'HR_adj_p:gene3',\n",
    "    'logFC:gene4', 'logFC_adj_p:gene4', 'HR:gene4', 'HR_adj_p:gene4'\n",
    "]\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = pd.ExcelFile(output_data)\n",
    "\n",
    "# Dictionary to store filtered data from each sheet\n",
    "filtered_data = {}\n",
    "consolidated_df = pd.DataFrame()  # DataFrame to store merged data\n",
    "\n",
    "# Iterate through each sheet\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    \n",
    "    # Check if all specified columns are present in the sheet\n",
    "    if all(col in df.columns for col in columns_to_check):\n",
    "        # Filter rows where all columns are non-empty\n",
    "        filtered_df = df[(df[columns_to_check] != 0).all(axis=1)]\n",
    "        \n",
    "        # Store the filtered data\n",
    "        filtered_data[sheet_name] = filtered_df\n",
    "\n",
    "        # Print the sheet name and the filtered data\n",
    "        print(f\"\\nFiltered data for sheet: {sheet_name}\")\n",
    "        if filtered_df.empty:\n",
    "            print(\"No rows found with all non-empty values.\")\n",
    "        else:\n",
    "            print(filtered_df)\n",
    "\n",
    "        # Append to the consolidated DataFrame\n",
    "        consolidated_df = pd.concat([consolidated_df, filtered_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Output filtered data or save to a new Excel file\n",
    "output_filtered_file = f\"{outputfolder}/filtered_LogFC_HR_OCC_CDG.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_filtered_file) as writer:\n",
    "    for sheet_name, filtered_df in filtered_data.items():\n",
    "        # Save each sheet to the new Excel file\n",
    "        filtered_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Save the consolidated data as the last sheet\n",
    "    consolidated_df.to_excel(writer, sheet_name=\"Consolidated\", index=False)\n",
    "\n",
    "\n",
    "print(\"\\nFiltered data saved to 'filtered_LogFC_HR_OCC_CDG.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c084387-e7f3-4501-8689-582a994b2e3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8645,
     "status": "ok",
     "timestamp": 1715326419138,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "3c084387-e7f3-4501-8689-582a994b2e3b",
    "outputId": "90d0680d-b5d9-41b0-87d7-891365a48ebe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f6650-77d7-4b8c-8447-df65edd7d945",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2165,
     "status": "ok",
     "timestamp": 1715934371483,
     "user": {
      "displayName": "aninda astuti",
      "userId": "13240177680153554239"
     },
     "user_tz": -480
    },
    "id": "511f6650-77d7-4b8c-8447-df65edd7d945",
    "outputId": "57a6c8b1-4d09-4b70-9fb7-455b4922686a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Input file path (the Excel file with one sheet)\n",
    "input_data = f\"{wkdr}/output files/combined-rank-score-combination.xlsx\"\n",
    "output_directory = f\"{wkdr}/output files\"\n",
    "\n",
    "# Read the single sheet from the Excel file\n",
    "processing_sheet = pd.read_excel(output_data)\n",
    "\n",
    "# Extract data for plotting\n",
    "x_ranking = processing_sheet.index + 1  # Ranking is based on index + 1\n",
    "\n",
    "y_Rank_SC_logFC = processing_sheet[\"flogFC\"]\n",
    "y_Rank_SC_HR = processing_sheet[\"fHR\"]\n",
    "y_Rank_SC_Occurences = processing_sheet[\"fOcc\"]\n",
    "y_Rank_SC_Cancer_Driver_Gene = processing_sheet[\"fCDG\"]\n",
    "# y_Rank_SC_PPIs = processing_sheet[\"fPPIs\"]  # Uncomment if needed\n",
    "\n",
    "# Calculate the maximum value of x-axis dynamically\n",
    "max_x_value = len(processing_sheet)\n",
    "\n",
    "# Create a plot for the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ranking, y_Rank_SC_logFC, color='blue', alpha=0.5, label='logFC', marker='o')\n",
    "plt.plot(x_ranking, y_Rank_SC_HR, color='green', alpha=0.5, label='HR', marker='s')\n",
    "plt.plot(x_ranking, y_Rank_SC_Occurences, color='red', alpha=0.5, label='Occ', marker='^')\n",
    "plt.plot(x_ranking, y_Rank_SC_Cancer_Driver_Gene, color='orange', alpha=0.5, label='CDG', marker='x')\n",
    "# plt.plot(x_ranking, y_Rank_SC_PPIs, color='purple', alpha=0.5, label='PPIs', marker='d')  # Uncomment if needed\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Rank-Score Characteristic Graph for 4-node Combined Data (All Subgraph ID)')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the maximum value of x-axis\n",
    "plt.xlim(right=max_x_value)\n",
    "\n",
    "# Set the x-axis ticks to show every nth value and adjust the font size\n",
    "step_size = 19  # Adjust this to increase/decrease the spacing between labels\n",
    "plt.xticks(range(1, max_x_value + 1, step_size), fontsize=8)  # Adjust 'fontsize' as needed\n",
    "\n",
    "\n",
    "# Save the plot to a file\n",
    "output_file = os.path.join(output_directory, \"combined_plot_RSC.jpg\")\n",
    "plt.savefig(output_file, format='jpg', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fba7f-cc1b-42f7-9391-ed0bd4465083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "# Define working directory and input/output paths\n",
    "input_data = f\"{wkdr}/output files/combined-rank-score-combination.xlsx\"\n",
    "output_directory = f\"{wkdr}/output files\"\n",
    "\n",
    "# Read the Excel file\n",
    "processing_sheet = pd.read_excel(input_data)\n",
    "\n",
    "# List of features\n",
    "features = [\"flogFC\", \"fHR\", \"fOcc\", \"fCDG\"]  # Include all available features\n",
    "\n",
    "# Generate combinations\n",
    "combinations_4_features = list(combinations(features, 4))\n",
    "combinations_3_features = list(combinations(features, 3))\n",
    "combinations_2_features = list(combinations(features, 2))\n",
    "\n",
    "# Combine all combinations into one list\n",
    "all_combinations = combinations_4_features + combinations_3_features + combinations_2_features\n",
    "\n",
    "# Define a color map for features\n",
    "feature_colors = {\n",
    "    \"flogFC\": \"blue\",\n",
    "    \"fHR\": \"green\",\n",
    "    \"fOcc\": \"red\",\n",
    "    \"fCDG\": \"orange\",\n",
    "    \"fPPIs\": \"purple\"  # Add more feature-color mappings as needed\n",
    "}\n",
    "\n",
    "# Loop through each combination and generate plots\n",
    "for idx, combination in enumerate(all_combinations, 1):\n",
    "    # Extract data for the current combination\n",
    "    x_ranking = processing_sheet.index + 1  # Ranking is based on index + 1\n",
    "    y_values = [processing_sheet[feature] for feature in combination]\n",
    "    \n",
    "    # Create a plot for the current combination\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot each feature in the combination\n",
    "    for y, feature in zip(y_values, combination):\n",
    "        plt.plot(\n",
    "            x_ranking, y, \n",
    "            color=feature_colors.get(feature, \"black\"),  # Default to black if no color is specified\n",
    "            alpha=0.5, \n",
    "            label=feature, \n",
    "            marker='o'  # You can customize markers per feature if needed\n",
    "        )\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f\"Rank-Score Characteristic Graph for Combination of ({', '.join(combination)}) Features\", fontsize=16)\n",
    "    plt.xlabel('Rank', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    \n",
    "    # Add legend and grid\n",
    "    plt.legend(fontsize=10, loc='upper right', frameon=True)\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "    \n",
    "    # Set the x-axis limits and ticks\n",
    "    max_x_value = len(processing_sheet)\n",
    "    plt.xlim(left=1, right=max_x_value)\n",
    "    step_size = 7  # Adjust this to increase/decrease spacing between x-axis labels\n",
    "    plt.xticks(range(1, max_x_value + 1, step_size), fontsize=10, rotation=45)\n",
    "    \n",
    "    # Save the plot\n",
    "    output_file = os.path.join(output_directory, f\"combined_plot_RSC_{idx}.jpg\")\n",
    "    plt.savefig(output_file, format='jpg', dpi=300)\n",
    "    \n",
    "    # Display the plot (optional)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab73667-6292-46f4-b40d-1bfc0e86d7c7",
   "metadata": {},
   "source": [
    "### CD and DS calculation for 11 features combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b3adc-7d6b-451d-9d13-de19302aca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Update paths and file names as required\n",
    "input_ranking = f\"{wkdr}/output files/combined-rank-score-combination.xlsx\"\n",
    "output_results = f\"{wkdr}/output files/weighted_calculation_results_combined_allsubgraph.xlsx\"\n",
    "\n",
    "# Load data\n",
    "rank_data = pd.read_excel(input_ranking, sheet_name=None)\n",
    "combined_data = pd.concat(rank_data.values(), ignore_index=True)\n",
    "\n",
    "# Initialize lists to store the results\n",
    "m = combined_data.shape[0]\n",
    "print(f\"The total number of rows in combined data is: {m}\")\n",
    "\n",
    "# Extract feature values and scores\n",
    "FC = combined_data[\"flogFC\"]\n",
    "HR = combined_data[\"fHR\"]\n",
    "occ = combined_data[\"fOcc\"]\n",
    "cdg = combined_data[\"fCDG\"]\n",
    "\n",
    "feat = 4\n",
    "\n",
    "# Extract score values\n",
    "s_FC = combined_data[\"slogFC'\"]\n",
    "s_HR = combined_data[\"sHR'\"]\n",
    "s_occ = combined_data[\"sOcc'\"]\n",
    "s_cdg = combined_data[\"sCDG'\"]\n",
    "\n",
    "# Calculate avg_sc\n",
    "#combined_data['avg_sc'] = (s_FC + s_HR + s_occ + s_cdg) / feat\n",
    "\n",
    "# Rank based on avg_sc\n",
    "#combined_data['rank_avg_sc'] = combined_data['avg_sc'].rank(ascending=False)\n",
    "\n",
    "# Print the scores\n",
    "print(\"s_FC :\", s_FC)\n",
    "\n",
    "# Cognitive Diversity (CD) Calculation\n",
    "CD_FC_HR = np.sqrt(np.sum((FC - HR) ** 2) / m)\n",
    "CD_FC_occ = np.sqrt(np.sum((FC - occ) ** 2) / m)\n",
    "CD_FC_cdg = np.sqrt(np.sum((FC - cdg) ** 2) / m)\n",
    "CD_HR_occ = np.sqrt(np.sum((HR - occ) ** 2) / m)\n",
    "CD_HR_cdg = np.sqrt(np.sum((HR - cdg) ** 2) / m)\n",
    "CD_occ_cdg = np.sqrt(np.sum((occ - cdg) ** 2) / m)\n",
    "\n",
    "def calculate_diversity_strength(n, features, CD_matrix):\n",
    "    ds = {}\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1, len(features)):\n",
    "            feature1 = features[i]\n",
    "            feature2 = features[j]\n",
    "            ds[feature1] = ds.get(feature1, 0) + CD_matrix.get((feature1, feature2), 0)\n",
    "            ds[feature2] = ds.get(feature2, 0) + CD_matrix.get((feature1, feature2), 0)\n",
    "\n",
    "    for key in ds:\n",
    "        ds[key] = ds[key] * (1 / (n - 1))\n",
    "\n",
    "    return ds\n",
    "\n",
    "def calculate_weighted_ds(row, ds, s_scores, features):\n",
    "    total_weighted_score = 0\n",
    "    sum_ds = 0\n",
    "\n",
    "    # Dynamically calculate the weighted score for the given features\n",
    "    for feature in features:\n",
    "        total_weighted_score += ds.get(feature, 0) * row[s_scores[feature]]\n",
    "        sum_ds += ds.get(feature, 0)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if sum_ds == 0:\n",
    "        return 0\n",
    "\n",
    "    return total_weighted_score / sum_ds\n",
    "\n",
    "# Define the feature set\n",
    "features = ['FC', 'HR', 'occ', 'cdg']\n",
    "\n",
    "# Define the Cognitive Diversity matrix for different feature pairs\n",
    "CD_matrix = {\n",
    "    ('FC', 'HR'): CD_FC_HR,\n",
    "    ('FC', 'occ'): CD_FC_occ,\n",
    "    ('FC', 'cdg'): CD_FC_cdg,\n",
    "    ('HR', 'occ'): CD_HR_occ,\n",
    "    ('HR', 'cdg'): CD_HR_cdg,\n",
    "    ('occ', 'cdg'): CD_occ_cdg\n",
    "}\n",
    "\n",
    "# Define the s_scores (these represent the actual column values from combined_data)\n",
    "s_scores = {\n",
    "    'FC': \"slogFC'\",\n",
    "    'HR': \"sHR'\",\n",
    "    'occ': \"sOcc'\",\n",
    "    'cdg': \"sCDG'\"\n",
    "}\n",
    "\n",
    "# Generate all combinations of 4, 3, and 2 features\n",
    "combinations_4_features = list(combinations(features, 4))\n",
    "combinations_3_features = list(combinations(features, 3))\n",
    "combinations_2_features = list(combinations(features, 2))\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results_dict = {}\n",
    "\n",
    "# Function to prepare the dataframe for each combination\n",
    "def create_combination_df(combo, CD_matrix, combined_data):\n",
    "    df = combined_data.copy()\n",
    "\n",
    "    # Add CD values\n",
    "    for i in range(len(combo)):\n",
    "        for j in range(i + 1, len(combo)):\n",
    "            pair = (combo[i], combo[j])\n",
    "            column_name = f\"CD_{combo[i]}_{combo[j]}\"\n",
    "            df[column_name] = CD_matrix.get(pair, 0)\n",
    "\n",
    "    # Add DS values for each feature\n",
    "    ds_result = calculate_diversity_strength(len(combo), combo, CD_matrix)\n",
    "    for feature in combo:\n",
    "        df[f\"ds_{feature}\"] = ds_result.get(feature, 0)\n",
    "\n",
    "    # Calculate weighted diversity strength for each row using the correct formula\n",
    "    df[\"weighted_ds\"] = df.apply(lambda row: calculate_weighted_ds(row, ds_result, s_scores, combo), axis=1)\n",
    "\n",
    "    # Rank the weighted_ds\n",
    "    df[\"weighted_ds_rank\"] = df[\"weighted_ds\"].rank(ascending=False, method='average')\n",
    "\n",
    "\n",
    "     # Calculate avg_sc based on the features in the current combination\n",
    "    score_columns = [s_scores[feature] for feature in combo]\n",
    "    df['avg_sc'] = df[score_columns].mean(axis=1)\n",
    "\n",
    "    # Rank avg_sc based on the calculated avg_sc for each row\n",
    "    df['rank_avg_sc'] = df['avg_sc'].rank(ascending=False, method='average')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process all feature combinations\n",
    "for combo in combinations_4_features + combinations_3_features + combinations_2_features:\n",
    "    sheet_name = \"_\".join(combo)\n",
    "    combo_df = create_combination_df(combo, CD_matrix, combined_data)\n",
    "    results_dict[sheet_name] = combo_df\n",
    "\n",
    "# Write the results to an Excel file with separate sheets for each combination\n",
    "with pd.ExcelWriter(output_results, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in results_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"Excel file with all combinations, CD/DS values, avg_sc, and rankings has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235190e9-504f-4a2b-becf-bd483f610d6b",
   "metadata": {},
   "source": [
    "## Module05: Find TOP2 and BOTTOM2 from Weigthed Score Combination result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f76ce-7707-4eba-8dc1-854869c3de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the working directory for saving the plots\n",
    "#wkdr = \"/Users/anindaastuti/CFA 4node combine features/output files\"\n",
    "\n",
    "# Function to plot the result for a specific feature combination and save the figure\n",
    "def plot_weighted_ds_vs_data(df, combination_name):\n",
    "    # Create a plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Add a column for data labels (d1, d2, ..., dn)\n",
    "    df['data'] = [f'd{i+1}' for i in range(len(df))]\n",
    "\n",
    "    # Sort the dataframe based on weighted diversity score rank to get top and bottom 2\n",
    "    df_sorted = df.sort_values(by=\"weighted_ds_rank\")\n",
    "\n",
    "    # Get the top 2 and bottom 2 ranked rows\n",
    "    top_2 = df_sorted.head(2)\n",
    "    bottom_2 = df_sorted.tail(2)\n",
    "\n",
    "    # Plot all the points using a default blue color\n",
    "    plt.scatter(df['data'], df['weighted_ds'], color='blue', label='Other ranks')\n",
    "\n",
    "    # Highlight the top 2 points with different colors\n",
    "    top1_data = top_2.iloc[0]['data']\n",
    "    top1_ds = top_2.iloc[0]['weighted_ds']\n",
    "    top2_data = top_2.iloc[1]['data']\n",
    "    top2_ds = top_2.iloc[1]['weighted_ds']\n",
    "    \n",
    "    plt.scatter(top1_data, top1_ds, color='red', label='Top 1 rank')\n",
    "    plt.scatter(top2_data, top2_ds, color='orange', label='Top 2 rank')\n",
    "\n",
    "    # Highlight the bottom 2 points with different colors\n",
    "    bottom1_data = bottom_2.iloc[0]['data']\n",
    "    bottom1_ds = bottom_2.iloc[0]['weighted_ds']\n",
    "    bottom2_data = bottom_2.iloc[1]['data']\n",
    "    bottom2_ds = bottom_2.iloc[1]['weighted_ds']\n",
    "    \n",
    "    plt.scatter(bottom1_data, bottom1_ds, color='brown', label='Bottom 1 rank')\n",
    "    plt.scatter(bottom2_data, bottom2_ds, color='green', label='Bottom 2 rank')\n",
    "\n",
    "    # Annotate the top 2 points with their data label\n",
    "    plt.annotate(f'{top1_data}', (top1_data, top1_ds),\n",
    "                 textcoords=\"offset points\", xytext=(0, 5), ha='center', color='red', fontsize=7)\n",
    "    plt.annotate(f'{top2_data}', (top2_data, top2_ds),\n",
    "                 textcoords=\"offset points\", xytext=(0, 5), ha='center', color='orange', fontsize=7)\n",
    "\n",
    "    # Annotate the bottom 2 points with their data label\n",
    "    plt.annotate(f'{bottom1_data}', (bottom1_data, bottom1_ds),\n",
    "                 textcoords=\"offset points\", xytext=(0, -10), ha='center', color='brown', fontsize=7)\n",
    "    plt.annotate(f'{bottom2_data}', (bottom2_data, bottom2_ds),\n",
    "                 textcoords=\"offset points\", xytext=(0, -10), ha='center', color='green', fontsize=7)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Samples Data (d1 to d114)')\n",
    "    plt.ylabel('Weighted_sc')\n",
    "    plt.title(f'Weighted Score Combination for {combination_name} Features')\n",
    "\n",
    "    # Customize x-axis labels with intervals of 5\n",
    "    plt.xticks(ticks=range(0, len(df), 5), labels=[f'd{i+1}' for i in range(0, len(df), 5)], rotation=90)\n",
    "\n",
    "    # Add a legend to explain the color scheme\n",
    "    plt.legend()\n",
    "\n",
    "    # Remove the grid background\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Adjust the plot to fit in the top-right corner\n",
    "    plt.xlim(left=0)  # Adjust as needed\n",
    "    plt.ylim(bottom=0)  # Adjust as needed\n",
    "\n",
    "    # Reduce padding to fit the plot area\n",
    "    plt.subplots_adjust(right=0.95, top=0.95, left=0.05, bottom=0.05)\n",
    "\n",
    "    # Save the plot with 300 DPI in the specified directory\n",
    "    filename = f\"{wkdr}/{combination_name}_plot.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Process all feature combinations and plot the results\n",
    "for combo in combinations_4_features + combinations_3_features + combinations_2_features:\n",
    "    # Get the sheet name based on the combination of features\n",
    "    sheet_name = \"_\".join(combo)\n",
    "\n",
    "    # Extract the DataFrame for this combination from the results_dict\n",
    "    combo_df = results_dict[sheet_name]\n",
    "\n",
    "    # Call the plotting function for this specific combination\n",
    "    plot_weighted_ds_vs_data(combo_df, sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b04cc-802c-496e-9344-3acb68aa2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the new file path for saving the top 2 and bottom 2 results\n",
    "new_excel_file = \"/Users/anindaastuti/CFA 4node combine features/output files/top_bottom_ranks.xlsx\"\n",
    "\n",
    "# Function to process the top and bottom 2 ranks for a specific feature combination and save to Excel\n",
    "def save_top_bottom_to_new_excel(df, combination_name, writer):\n",
    "    # Add a column for data labels (d1, d2, ..., dn)\n",
    "    df['data'] = [f'd{i+1}' for i in range(len(df))]\n",
    "\n",
    "    # Sort the dataframe based on weighted diversity score rank to get top and bottom 2\n",
    "    df_sorted = df.sort_values(by=\"weighted_ds_rank\")\n",
    "\n",
    "    # Get the top 2 and bottom 2 ranked rows\n",
    "    top_2 = df_sorted.head(2)\n",
    "    bottom_2 = df_sorted.tail(2)\n",
    "\n",
    "    # Combine the top and bottom 2 into one dataframe\n",
    "    top_bottom_df = pd.concat([top_2, bottom_2])\n",
    "\n",
    "    # Write the top and bottom 2 data to the new Excel file\n",
    "    top_bottom_df.to_excel(writer, sheet_name=f'Top_Bottom_{combination_name}', index=False)\n",
    "\n",
    "# Create a new Excel file and save the top/bottom 2 data for all combinations\n",
    "with pd.ExcelWriter(new_excel_file, engine='openpyxl') as writer:\n",
    "    for combo in combinations_4_features + combinations_3_features + combinations_2_features:\n",
    "        # Get the sheet name based on the combination of features\n",
    "        sheet_name = \"_\".join(combo)\n",
    "\n",
    "        # Extract the DataFrame for this combination from the results_dict\n",
    "        combo_df = results_dict[sheet_name]\n",
    "\n",
    "        # Call the function to save the top and bottom 2 for this combination to the new Excel file\n",
    "        save_top_bottom_to_new_excel(combo_df, sheet_name, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b6523-98a6-44ce-ae44-81f8cc2d78a4",
   "metadata": {},
   "source": [
    "## Module06: Calculate frequency item-ID TOP5 and BOTTOM5.\n",
    "It is chosen from the highest and lowest five ranked items-Id for each of the 11 combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01ee9c-7f18-4767-9fc1-0a4eca1c12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 5 or 10 and bottom 5 0r 10 frequency\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Define the new file path for saving the top 5 and bottom 5 results and summary\n",
    "new_excel_file = \"/Users/anindaastuti/CFA 4node combine features/output files/summary_top_bottom_10_subgraph_ids.xlsx\"\n",
    "\n",
    "# Initialize dictionaries to store top and bottom 5 subgraph IDs for each combination\n",
    "summary_top_dict = {}\n",
    "summary_bottom_dict = {}\n",
    "n_pick=10\n",
    "# Function to process the top and bottom 5 ranks for a specific feature combination\n",
    "def process_top_bottom_5_for_combination(df, combination_name):\n",
    "    # Sort the dataframe based on weighted diversity score rank to get top and bottom 5\n",
    "    df_sorted = df.sort_values(by=\"weighted_ds_rank\")\n",
    "    \n",
    "    top_5 = df_sorted.head(n_pick)  # Get the top 5\n",
    "    bottom_5 = df_sorted.tail(n_pick)  # Get the bottom 5\n",
    "\n",
    "    # Collect subgraph IDs from the 'data' column for top and bottom\n",
    "    top_subgraph_ids = top_5['data'].tolist()\n",
    "    bottom_subgraph_ids = bottom_5['data'].tolist()\n",
    "\n",
    "    # Store the top and bottom 5 subgraph IDs in the summary dictionaries\n",
    "    summary_top_dict[combination_name] = top_subgraph_ids\n",
    "    summary_bottom_dict[combination_name] = bottom_subgraph_ids\n",
    "\n",
    "    # Return both lists for frequency analysis\n",
    "    return top_subgraph_ids, bottom_subgraph_ids\n",
    "\n",
    "# Create a new Excel file and save the top/bottom 5 data for all combinations\n",
    "with pd.ExcelWriter(new_excel_file, engine='openpyxl') as writer:\n",
    "    all_top_ids = []     # To store all top 5 IDs across all combinations\n",
    "    all_bottom_ids = []  # To store all bottom 5 IDs across all combinations\n",
    "    \n",
    "    for combo in combinations_4_features + combinations_3_features + combinations_2_features:\n",
    "        # Get the sheet name based on the combination of features\n",
    "        sheet_name = \"_\".join(combo)\n",
    "\n",
    "        # Extract the DataFrame for this combination from the results_dict\n",
    "        combo_df = results_dict[sheet_name]\n",
    "\n",
    "        # Process the top and bottom 5 and store them\n",
    "        top_5_ids, bottom_5_ids = process_top_bottom_5_for_combination(combo_df, sheet_name)\n",
    "        \n",
    "        # Add the top and bottom 5 IDs to the global lists\n",
    "        all_top_ids.extend(top_5_ids)\n",
    "        all_bottom_ids.extend(bottom_5_ids)\n",
    "\n",
    "    # Calculate frequency of each subgraph ID across all combinations for both top and bottom\n",
    "    top_subgraph_count = Counter(all_top_ids)\n",
    "    bottom_subgraph_count = Counter(all_bottom_ids)\n",
    "\n",
    "    most_common_top_subgraphs = top_subgraph_count.most_common()\n",
    "    most_common_bottom_subgraphs = bottom_subgraph_count.most_common()\n",
    "\n",
    "    # Prepare summary DataFrames with the top and bottom 5 subgraph IDs for each combination\n",
    "    summary_top_df = pd.DataFrame(summary_top_dict)\n",
    "    summary_bottom_df = pd.DataFrame(summary_bottom_dict)\n",
    "\n",
    "    summary_top_df.index = [f'Top {i+1}' for i in range(n_pick)]     # Label the rows as Top 1, Top 2, ..., Top 5\n",
    "    summary_bottom_df.index = [f'Bottom {i+1}' for i in range(n_pick)]  # Label the rows as Bottom 1, Bottom 2, ..., Bottom 5\n",
    "\n",
    "    # Save the summary tables with top 5 and bottom 5 subgraph IDs for each combination\n",
    "    summary_top_df.to_excel(writer, sheet_name='Top_10_Subgraph_IDs_Summary')\n",
    "    summary_bottom_df.to_excel(writer, sheet_name='Bottom_10_Subgraph_IDs_Summary')\n",
    "\n",
    "    # Save the most common subgraph IDs across all combinations for both top and bottom\n",
    "    common_top_subgraphs_df = pd.DataFrame(most_common_top_subgraphs, columns=['Top_Subgraph_ID', 'Frequency_Top'])\n",
    "    common_bottom_subgraphs_df = pd.DataFrame(most_common_bottom_subgraphs, columns=['Bottom_Subgraph_ID', 'Frequency_Bottom'])\n",
    "\n",
    "    common_top_subgraphs_df.to_excel(writer, sheet_name='Most_Common_Top_Subgraph_IDs', index=False)\n",
    "    common_bottom_subgraphs_df.to_excel(writer, sheet_name='Most_Common_Bottom_Subgraph_IDs', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f27e33-91fe-4525-9882-694adb8c4125",
   "metadata": {},
   "source": [
    "## Module07: Calculate Jaccard index to Quantify the difference between the weighted SC and average SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b862b-f58e-46bc-a9ec-94c41e5b3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the top 2 and bottom 2 for Avg_Sc\n",
    "import pandas as pd\n",
    "\n",
    "# Define the new file path for saving the top 2 and bottom 2 results\n",
    "new_excel_file = \"/Users/anindaastuti/CFA 4node combine features/output files/Avg_Sc_top_bottom_ranks.xlsx\"\n",
    "\n",
    "# Function to process the top and bottom 2 ranks for a specific feature combination and save to Excel\n",
    "def save_top_bottom_to_new_excel(df, combination_name, writer):\n",
    "    # Add a column for data labels (d1, d2, ..., dn)\n",
    "    df['data'] = [f'd{i+1}' for i in range(len(df))]\n",
    "\n",
    "    # Sort the dataframe based on weighted diversity score rank to get top and bottom 2\n",
    "    df_sorted = df.sort_values(by=\"rank_avg_sc\")\n",
    "\n",
    "    # Get the top 2 and bottom 2 ranked rows\n",
    "    top_2 = df_sorted.head(2)\n",
    "    bottom_2 = df_sorted.tail(2)\n",
    "\n",
    "    # Combine the top and bottom 2 into one dataframe\n",
    "    top_bottom_df = pd.concat([top_2, bottom_2])\n",
    "\n",
    "    # Write the top and bottom 2 data to the new Excel file\n",
    "    top_bottom_df.to_excel(writer, sheet_name=f'Top_Bottom_{combination_name}', index=False)\n",
    "\n",
    "# Create a new Excel file and save the top/bottom 2 data for all combinations\n",
    "with pd.ExcelWriter(new_excel_file, engine='openpyxl') as writer:\n",
    "    for combo in combinations_4_features + combinations_3_features + combinations_2_features:\n",
    "        # Get the sheet name based on the combination of features\n",
    "        sheet_name = \"_\".join(combo)\n",
    "\n",
    "        # Extract the DataFrame for this combination from the results_dict\n",
    "        combo_df = results_dict[sheet_name]\n",
    "\n",
    "        # Call the function to save the top and bottom 2 for this combination to the new Excel file\n",
    "        save_top_bottom_to_new_excel(combo_df, sheet_name, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc9b2a-20df-454a-9abb-4a27b31de0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 5 and bottom 5 frequency\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Define the new file path for saving the top 5 and bottom 5 results and summary\n",
    "new_excel_file = \"/Users/anindaastuti/CFA 4node combine features/output files/avg_Sc_summary_top_bottom_10_subgraph_ids.xlsx\"\n",
    "\n",
    "# Initialize dictionaries to store top and bottom 5 subgraph IDs for each combination\n",
    "summary_top_dict = {}\n",
    "summary_bottom_dict = {}\n",
    "\n",
    "# Function to process the top and bottom 5 ranks for a specific feature combination\n",
    "def process_top_bottom_5_for_combination(df, combination_name):\n",
    "    # Sort the dataframe based on weighted diversity score rank to get top and bottom 5\n",
    "    df_sorted = df.sort_values(by=\"rank_avg_sc\")\n",
    "    \n",
    "    top_5 = df_sorted.head(n_pick)  # Get the top 5\n",
    "    bottom_5 = df_sorted.tail(n_pick)  # Get the bottom 5\n",
    "\n",
    "    # Collect subgraph IDs from the 'data' column for top and bottom\n",
    "    top_subgraph_ids = top_5['data'].tolist()\n",
    "    bottom_subgraph_ids = bottom_5['data'].tolist()\n",
    "\n",
    "    # Store the top and bottom 5 subgraph IDs in the summary dictionaries\n",
    "    summary_top_dict[combination_name] = top_subgraph_ids\n",
    "    summary_bottom_dict[combination_name] = bottom_subgraph_ids\n",
    "\n",
    "    # Return both lists for frequency analysis\n",
    "    return top_subgraph_ids, bottom_subgraph_ids\n",
    "\n",
    "# Create a new Excel file and save the top/bottom 5 data for all combinations\n",
    "with pd.ExcelWriter(new_excel_file, engine='openpyxl') as writer:\n",
    "    all_top_ids = []     # To store all top 5 IDs across all combinations\n",
    "    all_bottom_ids = []  # To store all bottom 5 IDs across all combinations\n",
    "    \n",
    "    for combo in combinations_4_features + combinations_3_features + combinations_2_features:\n",
    "        # Get the sheet name based on the combination of features\n",
    "        sheet_name = \"_\".join(combo)\n",
    "\n",
    "        # Extract the DataFrame for this combination from the results_dict\n",
    "        combo_df = results_dict[sheet_name]\n",
    "\n",
    "        # Process the top and bottom 5 and store them\n",
    "        top_5_ids, bottom_5_ids = process_top_bottom_5_for_combination(combo_df, sheet_name)\n",
    "        \n",
    "        # Add the top and bottom 5 IDs to the global lists\n",
    "        all_top_ids.extend(top_5_ids)\n",
    "        all_bottom_ids.extend(bottom_5_ids)\n",
    "\n",
    "    # Calculate frequency of each subgraph ID across all combinations for both top and bottom\n",
    "    top_subgraph_count = Counter(all_top_ids)\n",
    "    bottom_subgraph_count = Counter(all_bottom_ids)\n",
    "\n",
    "    most_common_top_subgraphs = top_subgraph_count.most_common()\n",
    "    most_common_bottom_subgraphs = bottom_subgraph_count.most_common()\n",
    "\n",
    "    # Prepare summary DataFrames with the top and bottom 5 subgraph IDs for each combination\n",
    "    summary_top_df = pd.DataFrame(summary_top_dict)\n",
    "    summary_bottom_df = pd.DataFrame(summary_bottom_dict)\n",
    "\n",
    "    summary_top_df.index = [f'Top {i+1}' for i in range(n_pick)]     # Label the rows as Top 1, Top 2, ..., Top 5\n",
    "    summary_bottom_df.index = [f'Bottom {i+1}' for i in range(n_pick)]  # Label the rows as Bottom 1, Bottom 2, ..., Bottom 5\n",
    "\n",
    "    # Save the summary tables with top 5 and bottom 5 subgraph IDs for each combination\n",
    "    summary_top_df.to_excel(writer, sheet_name='Top_10_Subgraph_IDs_Summary')\n",
    "    summary_bottom_df.to_excel(writer, sheet_name='Bottom_10_Subgraph_IDs_Summary')\n",
    "\n",
    "    # Save the most common subgraph IDs across all combinations for both top and bottom\n",
    "    common_top_subgraphs_df = pd.DataFrame(most_common_top_subgraphs, columns=['Top_Subgraph_ID', 'Frequency_Top'])\n",
    "    common_bottom_subgraphs_df = pd.DataFrame(most_common_bottom_subgraphs, columns=['Bottom_Subgraph_ID', 'Frequency_Bottom'])\n",
    "\n",
    "    common_top_subgraphs_df.to_excel(writer, sheet_name='Most_Common_Top_Subgraph_IDs', index=False)\n",
    "    common_bottom_subgraphs_df.to_excel(writer, sheet_name='Most_Common_Bottom_Subgraph_IDs', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38837457-67a6-4e38-a2f2-ef215cee40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel files\n",
    "file1 = f\"{wkdr}/avg_Sc_summary_top_bottom_10_subgraph_ids.xlsx\"\n",
    "file2 = f\"{wkdr}/summary_top_bottom_10_subgraph_ids.xlsx\"\n",
    "\n",
    "# Specify the sheet names to compare\n",
    "sheets_to_compare = [\"Top_10_Subgraph_IDs_Summary\", \"Bottom_10_Subgraph_IDs_Summary\"]\n",
    "\n",
    "# List of columns to compare (assuming these are the 4-feature combination columns)\n",
    "columns_to_compare = [\n",
    "    'FC_HR_occ_cdg', 'FC_HR_occ', 'FC_HR_cdg', 'FC_occ_cdg', \n",
    "    'HR_occ_cdg', 'FC_HR', 'FC_occ', 'FC_cdg', 'HR_occ', 'HR_cdg', 'occ_cdg'\n",
    "]\n",
    "\n",
    "# Function to calculate the Jaccard index between two sets\n",
    "def jaccard_index(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Dictionary to store results\n",
    "jaccard_results = {}\n",
    "\n",
    "# Loop through each sheet and calculate the Jaccard index for each column\n",
    "for sheet in sheets_to_compare:\n",
    "    # Load each sheet from both files\n",
    "    df1 = pd.read_excel(file1, sheet_name=sheet)\n",
    "    df2 = pd.read_excel(file2, sheet_name=sheet)\n",
    "    \n",
    "    # Calculate Jaccard index for each specified column in the sheet\n",
    "    sheet_results = {}\n",
    "    for col in columns_to_compare:\n",
    "        set1 = set(df1[col].dropna())  # Convert to set and remove NaN values\n",
    "        set2 = set(df2[col].dropna())  # Convert to set and remove NaN values\n",
    "        sheet_results[col] = jaccard_index(set1, set2)\n",
    "    \n",
    "    # Store results for the current sheet\n",
    "    jaccard_results[sheet] = sheet_results\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "result_df = pd.DataFrame(jaccard_results).reset_index()\n",
    "result_df.columns = ['Column', 'Jaccard_Index_Top_10', 'Jaccard_Index_Bottom_10']\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file = \"/Users/anindaastuti/CFA 4node combine features/output files/jaccard_index_results_by_sheet_top_bott_10.xlsx\"\n",
    "result_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Jaccard index results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dbe8d7",
   "metadata": {
    "id": "27dbe8d7"
   },
   "source": [
    "### THIS IS THE END OF THE PROGRAM, PLEASE CHECK THE FINAL RESULT FILE in output files folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239fa6ef",
   "metadata": {
    "id": "239fa6ef"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bc202-7282-4278-a506-96401c84e45f",
   "metadata": {
    "id": "bd4bc202-7282-4278-a506-96401c84e45f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
